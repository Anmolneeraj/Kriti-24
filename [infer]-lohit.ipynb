{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dece0b",
   "metadata": {
    "id": "Op9b0WdNy3uZ",
    "outputId": "8d6c1424-701c-48d3-b777-0ed575106de0",
    "papermill": {
     "duration": 0.011249,
     "end_time": "2024-02-09T18:40:01.302933",
     "exception": false,
     "start_time": "2024-02-09T18:40:01.291684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f20777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:40:01.327168Z",
     "iopub.status.busy": "2024-02-09T18:40:01.326218Z",
     "iopub.status.idle": "2024-02-09T18:40:23.631044Z",
     "shell.execute_reply": "2024-02-09T18:40:23.629908Z"
    },
    "id": "4vLrBjS9FCIb",
    "papermill": {
     "duration": 22.321077,
     "end_time": "2024-02-09T18:40:23.634290",
     "exception": false,
     "start_time": "2024-02-09T18:40:01.313213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 18:40:06.682051: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-09 18:40:06.682305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-09 18:40:06.882051: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU,SimpleRNN\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "# from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9580d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:40:23.659395Z",
     "iopub.status.busy": "2024-02-09T18:40:23.657835Z",
     "iopub.status.idle": "2024-02-09T18:40:25.206981Z",
     "shell.execute_reply": "2024-02-09T18:40:25.205431Z"
    },
    "id": "LsmKDTYqwIBl",
    "papermill": {
     "duration": 1.56579,
     "end_time": "2024-02-09T18:40:25.210607",
     "exception": false,
     "start_time": "2024-02-09T18:40:23.644817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv('/kaggle/input/kriti2024/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbd1f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:40:25.234226Z",
     "iopub.status.busy": "2024-02-09T18:40:25.233776Z",
     "iopub.status.idle": "2024-02-09T18:40:25.367718Z",
     "shell.execute_reply": "2024-02-09T18:40:25.366047Z"
    },
    "id": "Tabx0o-j4M2f",
    "outputId": "d80d65ba-450c-4aea-ec34-c6f704bd2799",
    "papermill": {
     "duration": 0.149202,
     "end_time": "2024-02-09T18:40:25.370926",
     "exception": false,
     "start_time": "2024-02-09T18:40:25.221724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>CONTEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LO']</td>\n",
       "      <td>Axiomatic Aspects of Default Inference. This p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['math.GR']</td>\n",
       "      <td>On extensions of group with infinite conjugacy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.IT', 'eess.SP', 'math.IT']</td>\n",
       "      <td>An Analysis of Complex-Valued CNNs for RF Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['math.PR', 'math.ST', 'stat.TH']</td>\n",
       "      <td>On the reconstruction of the drift of a diffus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.IT', 'math.IT']</td>\n",
       "      <td>Three classes of propagation rules for GRS and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51205</th>\n",
       "      <td>['math.AP']</td>\n",
       "      <td>Generalized Fourier Integral Operators on spac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51206</th>\n",
       "      <td>['cs.CV', 'cs.CL']</td>\n",
       "      <td>Weakly-Supervised 3D Visual Grounding based on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51207</th>\n",
       "      <td>['math.CV']</td>\n",
       "      <td>Strongly pseudoconvex handlebodies. We give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51208</th>\n",
       "      <td>['cs.CL', 'cs.LG', 'cs.SD', 'eess.AS']</td>\n",
       "      <td>Improving End-to-End Speech Processing by Effi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51209</th>\n",
       "      <td>['math.PR']</td>\n",
       "      <td>Second class particles as microscopic characte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51210 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Categories  \\\n",
       "0                                     ['cs.LO']   \n",
       "1                                   ['math.GR']   \n",
       "2      ['cs.LG', 'cs.IT', 'eess.SP', 'math.IT']   \n",
       "3             ['math.PR', 'math.ST', 'stat.TH']   \n",
       "4                          ['cs.IT', 'math.IT']   \n",
       "...                                         ...   \n",
       "51205                               ['math.AP']   \n",
       "51206                        ['cs.CV', 'cs.CL']   \n",
       "51207                               ['math.CV']   \n",
       "51208    ['cs.CL', 'cs.LG', 'cs.SD', 'eess.AS']   \n",
       "51209                               ['math.PR']   \n",
       "\n",
       "                                                 CONTEXT  \n",
       "0      Axiomatic Aspects of Default Inference. This p...  \n",
       "1      On extensions of group with infinite conjugacy...  \n",
       "2      An Analysis of Complex-Valued CNNs for RF Data...  \n",
       "3      On the reconstruction of the drift of a diffus...  \n",
       "4      Three classes of propagation rules for GRS and...  \n",
       "...                                                  ...  \n",
       "51205  Generalized Fourier Integral Operators on spac...  \n",
       "51206  Weakly-Supervised 3D Visual Grounding based on...  \n",
       "51207  Strongly pseudoconvex handlebodies. We give an...  \n",
       "51208  Improving End-to-End Speech Processing by Effi...  \n",
       "51209  Second class particles as microscopic characte...  \n",
       "\n",
       "[51210 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['CONTEXT'] = train_df['Title'] + \". \" + train_df['Abstract']\n",
    "train_df.drop(labels=['Title', 'Abstract', 'Id'], axis=1, inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4963e368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:40:25.397193Z",
     "iopub.status.busy": "2024-02-09T18:40:25.396018Z",
     "iopub.status.idle": "2024-02-09T18:40:25.402753Z",
     "shell.execute_reply": "2024-02-09T18:40:25.401446Z"
    },
    "id": "cxtsvH_fkv62",
    "papermill": {
     "duration": 0.023318,
     "end_time": "2024-02-09T18:40:25.405917",
     "exception": false,
     "start_time": "2024-02-09T18:40:25.382599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seqlen = 250\n",
    "batch_size = 256\n",
    "padding_token = \"<pad>\"\n",
    "auto = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29169f4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:40:25.431042Z",
     "iopub.status.busy": "2024-02-09T18:40:25.430552Z",
     "iopub.status.idle": "2024-02-09T18:41:33.387052Z",
     "shell.execute_reply": "2024-02-09T18:41:33.385307Z"
    },
    "id": "rq64TcSwWGR2",
    "papermill": {
     "duration": 67.973465,
     "end_time": "2024-02-09T18:41:33.390622",
     "exception": false,
     "start_time": "2024-02-09T18:40:25.417157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def decontract(sentence):\n",
    "    # specific\n",
    "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n",
    "\n",
    "    # general\n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "    return sentence\n",
    "\n",
    "def cleanPunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', '', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\"\", sentence)\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stopwords) + \")\\\\W\", re.I)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "train_df['CONTEXT']=train_df['CONTEXT'].str.lower()\n",
    "train_df['CONTEXT']=train_df['CONTEXT'].apply(decontract)\n",
    "train_df['CONTEXT']=train_df['CONTEXT'].apply(cleanPunc)\n",
    "train_df['CONTEXT']=train_df['CONTEXT'].apply(removeStopWords)\n",
    "train_df['CONTEXT']=train_df['CONTEXT'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee52091f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:41:33.417458Z",
     "iopub.status.busy": "2024-02-09T18:41:33.416920Z",
     "iopub.status.idle": "2024-02-09T18:41:33.423677Z",
     "shell.execute_reply": "2024-02-09T18:41:33.422205Z"
    },
    "id": "at3uiAAnSCSy",
    "outputId": "4960c1a6-4cff-4ddd-b3a4-dd7d734a6c5a",
    "papermill": {
     "duration": 0.025282,
     "end_time": "2024-02-09T18:41:33.427312",
     "exception": false,
     "start_time": "2024-02-09T18:41:33.402030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from collections import Counter\n",
    "\n",
    "\n",
    "# # Tokenize the sentences into words and create a list of all words\n",
    "# all_words = []\n",
    "# for sentence in train_df['CONTEXT']:\n",
    "#     words = word_tokenize(sentence)\n",
    "#     all_words.extend(words)\n",
    "\n",
    "# # Count the frequency of each word\n",
    "# word_freq = Counter(all_words)\n",
    "\n",
    "# # Get the top 10 words\n",
    "# top_10_words = word_freq.most_common(10)\n",
    "\n",
    "# # Convert the result to a DataFrame for easier display\n",
    "# top_10_words_df = pd.DataFrame(top_10_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "# # Display the top 10 words and their frequencies\n",
    "# print(top_10_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67feb39b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:41:33.452702Z",
     "iopub.status.busy": "2024-02-09T18:41:33.451978Z",
     "iopub.status.idle": "2024-02-09T18:41:33.457177Z",
     "shell.execute_reply": "2024-02-09T18:41:33.455835Z"
    },
    "id": "OFO1caMCScwV",
    "papermill": {
     "duration": 0.020477,
     "end_time": "2024-02-09T18:41:33.460169",
     "exception": false,
     "start_time": "2024-02-09T18:41:33.439692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def remove_top_words(sentence):\n",
    "#     words = word_tokenize(sentence)\n",
    "#     words = [word for word in words if word not in top_10_words]\n",
    "#     return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2afefd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:41:33.484765Z",
     "iopub.status.busy": "2024-02-09T18:41:33.484300Z",
     "iopub.status.idle": "2024-02-09T18:41:33.489570Z",
     "shell.execute_reply": "2024-02-09T18:41:33.488132Z"
    },
    "id": "PueXH751X397",
    "outputId": "43c2e712-84ec-4354-f267-473d72a23dc2",
    "papermill": {
     "duration": 0.02027,
     "end_time": "2024-02-09T18:41:33.491975",
     "exception": false,
     "start_time": "2024-02-09T18:41:33.471705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_df['CONTEXT']=train_df['CONTEXT'].apply(remove_top_words)\n",
    "# train_df['CONTEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e2486c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:41:33.516642Z",
     "iopub.status.busy": "2024-02-09T18:41:33.515890Z",
     "iopub.status.idle": "2024-02-09T18:42:01.727793Z",
     "shell.execute_reply": "2024-02-09T18:42:01.726120Z"
    },
    "id": "PMzTRUrCyIUC",
    "outputId": "3b1d4a3a-b078-433b-f0df-eeed889232f8",
    "papermill": {
     "duration": 28.228731,
     "end_time": "2024-02-09T18:42:01.731824",
     "exception": false,
     "start_time": "2024-02-09T18:41:33.503093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 40495\n",
      "Number of rows in validation set: 5062\n",
      "Number of rows in test set: 5062\n",
      "58\n",
      "Abstract: b'estimate wave equation applications nonlinear problems prove estimates solutions cauchy problem inhomogeneous wave equation rn class banach spaces whose norms depend size spacetime fourier transform estimates local time allows one essentially replace symbol wave operator vanishes light cone fourier space inhomogeneous symbol inverted result improves earlier estimates type proved klainermanmachedon corollary one obtains rather general result concerning local wellposedness nonlinear wave equations'\n",
      "Label(s): ['math.AP']\n",
      " \n",
      "Abstract: b'combinatorial interpretation fibonomial coefficients combinatorial interpretation fibonomial coefficients number choices specific finite subsets infinite partially ordered set not binomial type proposed partially ordered set defined via characteristic matrix corresponding partial order relation relevance proposal general unification treatment indicated'\n",
      "Label(s): ['math.CO', 'cs.DM']\n",
      " \n",
      "Abstract: b'peerpeer deep learning beyondg iot present ppl practical multidevice peerpeer deep learning algorithm unlike federated learning paradigm not require coordination edge servers cloud makes ppl wellsuited sheer scale beyondg computing environments like smart cities otherwise create range latency bandwidth single point failure issues federated approaches ppl introduces max norm synchronization catalyze training retains device deep model training preserve privacy leverages local interdevice communication implement distributed consensus device iteratively alternates two phases  device learning  distributed cooperation combine model parameters nearby devices empirically show participating devices achieve test performance attained federated centralized training  even  devices relaxed singly stochastic consensus weights extend experimental results settings diverse network topologies sparse intermittent communication noniid data distributions'\n",
      "Label(s): ['cs.LG', 'cs.DC']\n",
      " \n",
      "Abstract: b'residues formulae volumes ehrhart polynomials convex polytopes notes explain residue formulae volumes convex polytopes ehrahrt polynomials based notion total residue apply method computation volume chanrobbins polytope final computation based total residue formula system an similar morris identity flow polytopes formula change variables total residues leads nice formula ehrhart polynomials function mixed volumes apply pitmanstanley polytope'\n",
      "Label(s): ['math.CO']\n",
      " \n",
      "Abstract: b'differential equation satisfied plane curve degree n eliminating arbitrary coefficients equation generic plane curve order n computing sufficiently many derivatives one obtains differential equation projective invariant first one corresponding conics obtained monge sylvester halphen cartan used invariants higher order expression invariants rather complicated becomes much simpler interpreted terms symmetric functions'\n",
      "Label(s): ['math.CO']\n",
      " \n",
      "112354\n",
      "(256, 112354) (256, 58)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df.reset_index(inplace = True)\n",
    "train_df['Categories'] = train_df['Categories'].str.split(',Â ' , expand = True)\n",
    "\n",
    "\n",
    "test_df  = pd.read_csv('/kaggle/input/kriti2024/test.csv')\n",
    "\n",
    "#drop off the categories with 1 occurence\n",
    "train_df_filtered = train_df.groupby(\"Categories\").filter(lambda x: len(x) > 1)\n",
    "\n",
    "\n",
    "train_df_filtered[\"Categories\"] = train_df_filtered[\"Categories\"].apply(\n",
    "    lambda x: literal_eval(x)\n",
    ")\n",
    "\n",
    "test_split = 0.2\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    train_df_filtered,\n",
    "    test_size=test_split,\n",
    "    stratify=train_df_filtered[\"Categories\"].values,\n",
    ")\n",
    "\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")\n",
    "\n",
    "terms = tf.ragged.constant(train_df[\"Categories\"].values)\n",
    "lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "lookup.adapt(terms)\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "\n",
    "def invert_multi_hot(encoded_labels):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a list of vocab terms.\"\"\"\n",
    "    hot_indices = np.where(encoded_labels > 0.4)[0]\n",
    "    terms = np.take(vocab, hot_indices, mode='clip')  # Use 'clip' mode to handle out-of-bounds indices\n",
    "    return terms.tolist()\n",
    "\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    labels = tf.ragged.constant(dataframe[\"Categories\"].values)\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dataframe[\"CONTEXT\"].values, label_binarized)\n",
    "    )\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)\n",
    "\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    print(\" \")\n",
    "\n",
    "vocabulary = set()\n",
    "train_df[\"CONTEXT\"].str.lower().str.split().apply(vocabulary.update)\n",
    "# train_df[\"Abstract\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)\n",
    "\n",
    "text_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf_idf\"\n",
    ")\n",
    "\n",
    "# `TextVectorization` layer needs to be adapted as per the vocabulary from our\n",
    "# training set.\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
    ").prefetch(auto)\n",
    "validation_dataset = validation_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
    ").prefetch(auto)\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
    ").prefetch(auto)\n",
    "\n",
    "train_batch,label = next(iter(train_dataset))\n",
    "print(train_batch.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8d03b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:42:01.758625Z",
     "iopub.status.busy": "2024-02-09T18:42:01.757898Z",
     "iopub.status.idle": "2024-02-09T18:42:01.776287Z",
     "shell.execute_reply": "2024-02-09T18:42:01.774935Z"
    },
    "id": "n9rTOOmoVRri",
    "outputId": "663bdbe9-3136-432f-a929-0b39009e141c",
    "papermill": {
     "duration": 0.035339,
     "end_time": "2024-02-09T18:42:01.779322",
     "exception": false,
     "start_time": "2024-02-09T18:42:01.743983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Categories</th>\n",
       "      <th>CONTEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51018</th>\n",
       "      <td>51018</td>\n",
       "      <td>[cs.AI]</td>\n",
       "      <td>model managing collections patterns data minin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14239</th>\n",
       "      <td>14239</td>\n",
       "      <td>[cs.DM]</td>\n",
       "      <td>much hamiltonian cycle weigh paper investigate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16682</th>\n",
       "      <td>16682</td>\n",
       "      <td>[math.AP]</td>\n",
       "      <td>relaxation approach minimisation neohookean en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14921</th>\n",
       "      <td>14921</td>\n",
       "      <td>[math.AP]</td>\n",
       "      <td>ray transform sobolev spaces symmetric tensor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51141</th>\n",
       "      <td>51141</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>maximum entropy approach identifying sentence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41325</th>\n",
       "      <td>41325</td>\n",
       "      <td>[cs.LG, cs.AI, cs.CV, q-bio.NC]</td>\n",
       "      <td>critical learning periods multisensory integra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8892</th>\n",
       "      <td>8892</td>\n",
       "      <td>[math.AP]</td>\n",
       "      <td>model efficient operation bank insurance compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9237</th>\n",
       "      <td>9237</td>\n",
       "      <td>[math.CO, math.NT]</td>\n",
       "      <td>counting permutations congruence class major i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40085</th>\n",
       "      <td>40085</td>\n",
       "      <td>[eess.SP]</td>\n",
       "      <td>timebased vs fingerprintingbased positioning u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>14637</td>\n",
       "      <td>[math.AP]</td>\n",
       "      <td>divergence form operators reifenberg flat doma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40495 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                       Categories  \\\n",
       "51018  51018                          [cs.AI]   \n",
       "14239  14239                          [cs.DM]   \n",
       "16682  16682                        [math.AP]   \n",
       "14921  14921                        [math.AP]   \n",
       "51141  51141                          [cs.CL]   \n",
       "...      ...                              ...   \n",
       "41325  41325  [cs.LG, cs.AI, cs.CV, q-bio.NC]   \n",
       "8892    8892                        [math.AP]   \n",
       "9237    9237               [math.CO, math.NT]   \n",
       "40085  40085                        [eess.SP]   \n",
       "14637  14637                        [math.AP]   \n",
       "\n",
       "                                                 CONTEXT  \n",
       "51018  model managing collections patterns data minin...  \n",
       "14239  much hamiltonian cycle weigh paper investigate...  \n",
       "16682  relaxation approach minimisation neohookean en...  \n",
       "14921  ray transform sobolev spaces symmetric tensor ...  \n",
       "51141  maximum entropy approach identifying sentence ...  \n",
       "...                                                  ...  \n",
       "41325  critical learning periods multisensory integra...  \n",
       "8892   model efficient operation bank insurance compa...  \n",
       "9237   counting permutations congruence class major i...  \n",
       "40085  timebased vs fingerprintingbased positioning u...  \n",
       "14637  divergence form operators reifenberg flat doma...  \n",
       "\n",
       "[40495 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1899e05e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:42:01.809442Z",
     "iopub.status.busy": "2024-02-09T18:42:01.808946Z",
     "iopub.status.idle": "2024-02-09T18:42:01.825046Z",
     "shell.execute_reply": "2024-02-09T18:42:01.823763Z"
    },
    "id": "CGiF7w1XyBdZ",
    "papermill": {
     "duration": 0.034858,
     "end_time": "2024-02-09T18:42:01.827839",
     "exception": false,
     "start_time": "2024-02-09T18:42:01.792981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "553f27cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T18:42:01.856190Z",
     "iopub.status.busy": "2024-02-09T18:42:01.855698Z",
     "iopub.status.idle": "2024-02-09T19:53:39.167100Z",
     "shell.execute_reply": "2024-02-09T19:53:39.165451Z"
    },
    "id": "K718Dr0H4O6a",
    "outputId": "b017213a-a20b-4a70-e4ac-9b2f83df1b71",
    "papermill": {
     "duration": 4297.511782,
     "end_time": "2024-02-09T19:53:39.352288",
     "exception": false,
     "start_time": "2024-02-09T18:42:01.840506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "159/159 [==============================] - 412s 3s/step - loss: 0.1345 - accuracy: 0.3090 - precision: 0.3538 - recall: 0.2756 - val_loss: 0.1025 - val_accuracy: 0.4970 - val_precision: 0.5838 - val_recall: 0.6987\n",
      "Epoch 2/10\n",
      "159/159 [==============================] - 408s 3s/step - loss: 0.0627 - accuracy: 0.5106 - precision: 0.7678 - recall: 0.5345 - val_loss: 0.0594 - val_accuracy: 0.5409 - val_precision: 0.7027 - val_recall: 0.6734\n",
      "Epoch 3/10\n",
      "159/159 [==============================] - 415s 3s/step - loss: 0.0446 - accuracy: 0.5903 - precision: 0.8247 - recall: 0.6757 - val_loss: 0.0551 - val_accuracy: 0.5650 - val_precision: 0.7347 - val_recall: 0.6938\n",
      "Epoch 4/10\n",
      "159/159 [==============================] - 414s 3s/step - loss: 0.0347 - accuracy: 0.6299 - precision: 0.8617 - recall: 0.7559 - val_loss: 0.0572 - val_accuracy: 0.5476 - val_precision: 0.7291 - val_recall: 0.7006\n",
      "Epoch 5/10\n",
      "159/159 [==============================] - 408s 3s/step - loss: 0.0285 - accuracy: 0.6511 - precision: 0.8872 - recall: 0.8055 - val_loss: 0.0622 - val_accuracy: 0.5695 - val_precision: 0.7410 - val_recall: 0.6935\n",
      "Epoch 6/10\n",
      "159/159 [==============================] - 410s 3s/step - loss: 0.0240 - accuracy: 0.6614 - precision: 0.9010 - recall: 0.8378 - val_loss: 0.0629 - val_accuracy: 0.5695 - val_precision: 0.7194 - val_recall: 0.7298\n",
      "Epoch 7/10\n",
      "159/159 [==============================] - 409s 3s/step - loss: 0.0206 - accuracy: 0.6688 - precision: 0.9177 - recall: 0.8652 - val_loss: 0.0660 - val_accuracy: 0.5766 - val_precision: 0.7404 - val_recall: 0.7085\n",
      "Epoch 8/10\n",
      "159/159 [==============================] - 408s 3s/step - loss: 0.0183 - accuracy: 0.6719 - precision: 0.9252 - recall: 0.8814 - val_loss: 0.0692 - val_accuracy: 0.5522 - val_precision: 0.7305 - val_recall: 0.7170\n",
      "Epoch 9/10\n",
      "159/159 [==============================] - 415s 3s/step - loss: 0.0164 - accuracy: 0.6719 - precision: 0.9328 - recall: 0.8967 - val_loss: 0.0700 - val_accuracy: 0.5632 - val_precision: 0.7319 - val_recall: 0.7270\n",
      "Epoch 10/10\n",
      "159/159 [==============================] - 408s 3s/step - loss: 0.0150 - accuracy: 0.6808 - precision: 0.9392 - recall: 0.9071 - val_loss: 0.0756 - val_accuracy: 0.5830 - val_precision: 0.7298 - val_recall: 0.7189\n",
      "CPU times: user 2h 59min 3s, sys: 1h 10min 44s, total: 4h 9min 48s\n",
      "Wall time: 1h 11min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# import keras.optimizers\n",
    "initializer = tf.keras.initializers.HeUniform( seed = 223)\n",
    "initializer2 = tf.keras.initializers.HeNormal( seed = 123)\n",
    "final_initializer = tf.keras.initializers.GlorotUniform(seed = 425)\n",
    "def make_model():\n",
    "    shallow_mlp_model = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(1024, kernel_initializer = initializer,activation=\"swish\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.6),\n",
    "            layers.Dense(512,kernel_initializer = initializer,  activation=\"swish\"),\n",
    "            # layers.BatchNormalization(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(256,kernel_initializer = initializer, activation=\"swish\"),\n",
    "            # layers.BatchNormalization(),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(128, kernel_initializer = initializer2, activation=\"leaky_relu\"),\n",
    "            layers.Dense(lookup.vocabulary_size(),kernel_initializer = final_initializer, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    return shallow_mlp_model\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "shallow_mlp_model = make_model()\n",
    "shallow_mlp_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",  Precision(), Recall()]\n",
    ")\n",
    "\n",
    "# shallow_mlp_model.compile(\n",
    "#     loss=HammingLoss(mode='multilabel'),\n",
    "#     optimizer=\"adam\",\n",
    "#     metrics=[\"accuracy\", \"binary_accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "# )\n",
    "history = shallow_mlp_model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae86d17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:53:39.718165Z",
     "iopub.status.busy": "2024-02-09T19:53:39.717727Z",
     "iopub.status.idle": "2024-02-09T19:53:53.211135Z",
     "shell.execute_reply": "2024-02-09T19:53:53.209533Z"
    },
    "id": "HnGs6CXz5grp",
    "outputId": "d7f3b4d6-b5da-4bd8-b824-8d6f07024756",
    "papermill": {
     "duration": 13.676293,
     "end_time": "2024-02-09T19:53:53.213958",
     "exception": false,
     "start_time": "2024-02-09T19:53:39.537665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 13s 658ms/step - loss: 0.0734 - accuracy: 0.5774 - precision: 0.7317 - recall: 0.7342\n",
      "  Test Loss: 0.0734\n",
      "Test binary accuracy: 0.5774\n",
      "Test Precision: 0.7317, Test Recall: 0.7342\n",
      "0.7329338917437435\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_bin_acc, test_precision, test_recall = shallow_mlp_model.evaluate(test_dataset)\n",
    "print(f'  Test Loss: {test_loss:.4f}')\n",
    "print(f\"Test binary accuracy: {test_bin_acc:.4f}\")\n",
    "print(f'Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}')\n",
    "f1 = (2 * test_precision * test_recall) / (test_precision + test_recall)\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b681c93a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:53:53.562611Z",
     "iopub.status.busy": "2024-02-09T19:53:53.561754Z",
     "iopub.status.idle": "2024-02-09T19:54:08.279487Z",
     "shell.execute_reply": "2024-02-09T19:54:08.278098Z"
    },
    "id": "EMOfRk7THLDo",
    "papermill": {
     "duration": 14.898006,
     "end_time": "2024-02-09T19:54:08.282763",
     "exception": false,
     "start_time": "2024-02-09T19:53:53.384757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "test_df = pd.read_csv('/kaggle/input/kriti2024/test.csv')\n",
    "\n",
    "def decontract(sentence):\n",
    "    # specific\n",
    "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n",
    "\n",
    "    # general\n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "    return sentence\n",
    "\n",
    "def cleanPunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', '', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\"\", sentence)\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stopwords) + \")\\\\W\", re.I)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "test_df['CONTEXT'] = test_df['Title'] + \". \" + test_df['Abstract']\n",
    "test_df.drop(labels=['Title', 'Abstract'], axis=1, inplace=True)\n",
    "\n",
    "test_df['CONTEXT']=test_df['CONTEXT'].str.lower()\n",
    "test_df['CONTEXT']=test_df['CONTEXT'].apply(decontract)\n",
    "test_df['CONTEXT']=test_df['CONTEXT'].apply(cleanPunc)\n",
    "test_df['CONTEXT']=test_df['CONTEXT'].apply(removeStopWords)\n",
    "test_df['CONTEXT']=test_df['CONTEXT'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "745d08ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:08.743209Z",
     "iopub.status.busy": "2024-02-09T19:54:08.742707Z",
     "iopub.status.idle": "2024-02-09T19:54:08.747752Z",
     "shell.execute_reply": "2024-02-09T19:54:08.746822Z"
    },
    "id": "ra3Ud-35q0dU",
    "outputId": "75ee5b43-05ec-43e0-b477-694e011512e3",
    "papermill": {
     "duration": 0.185725,
     "end_time": "2024-02-09T19:54:08.750335",
     "exception": false,
     "start_time": "2024-02-09T19:54:08.564610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_df['CONTEXT']=test_df['CONTEXT'].apply(remove_top_words)\n",
    "# test_df['CONTEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d25c1944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:09.094989Z",
     "iopub.status.busy": "2024-02-09T19:54:09.093651Z",
     "iopub.status.idle": "2024-02-09T19:54:46.999284Z",
     "shell.execute_reply": "2024-02-09T19:54:46.997611Z"
    },
    "id": "Krm8gz8sGUkC",
    "outputId": "d682835a-457b-494d-933e-e1ea4f40fb73",
    "papermill": {
     "duration": 38.080036,
     "end_time": "2024-02-09T19:54:47.002856",
     "exception": false,
     "start_time": "2024-02-09T19:54:08.922820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 37s 840ms/step\n"
     ]
    }
   ],
   "source": [
    "#Predicitions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load the test dataset\n",
    "\n",
    "\n",
    "# Preprocess the test dataset\n",
    "test_df['Categories'] = np.nan  # Placeholder for predicted categories\n",
    "\n",
    "#text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
    "# Assuming train_df is your training dataframe with \"Abstract\" column\n",
    "#text_vectorizer.adapt(tf.data.Dataset.from_tensor_slices(test_df[\"Abstract\"].values).batch(batch_size))\n",
    "# train_df[\"Abstract\"].str.lower().str.split().apply(vocabulary.update)\n",
    "# train_df[\"Title\"].str.lower().str.split().apply(vocabulary.update)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_df[\"CONTEXT\"].values )\n",
    "test_dataset = test_dataset.map(lambda text: text_vectorizer(text)).batch(batch_size)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = shallow_mlp_model.predict(test_dataset)\n",
    "\n",
    "# Convert predicted binary labels to categories using list comprehension\n",
    "predicted_categories = [invert_multi_hot(row) for row in predictions]\n",
    "\n",
    "# Convert the list of lists to a numpy array\n",
    "predicted_categories = np.array(predicted_categories, dtype=object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c032585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:47.370491Z",
     "iopub.status.busy": "2024-02-09T19:54:47.369980Z",
     "iopub.status.idle": "2024-02-09T19:54:47.377990Z",
     "shell.execute_reply": "2024-02-09T19:54:47.376619Z"
    },
    "id": "SJQW8azh5AR_",
    "outputId": "37c81fa7-8dde-4805-d884-b627fbe1a4ee",
    "papermill": {
     "duration": 0.192967,
     "end_time": "2024-02-09T19:54:47.380681",
     "exception": false,
     "start_time": "2024-02-09T19:54:47.187714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10974,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc01c27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:47.748645Z",
     "iopub.status.busy": "2024-02-09T19:54:47.747675Z",
     "iopub.status.idle": "2024-02-09T19:54:47.756825Z",
     "shell.execute_reply": "2024-02-09T19:54:47.755316Z"
    },
    "id": "szFthszomHTd",
    "papermill": {
     "duration": 0.19431,
     "end_time": "2024-02-09T19:54:47.759610",
     "exception": false,
     "start_time": "2024-02-09T19:54:47.565300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "desired_column_order = [\n",
    "    'Id', 'math.AT', 'stat.AP', 'cs.AR', 'math.QA', 'q-bio.MN', 'eess.AS', 'eess.IV', 'stat.ME',\n",
    "    'econ.GN', 'eess.SP', 'q-fin.RM', 'cs.LG', 'cs.CR', 'q-bio.BM', 'q-fin.GN', 'q-fin.MF',\n",
    "    'q-fin.PR', 'math.CV', 'cs.LO', 'econ.TH', 'math.CO', 'cs.AI', 'math.AC', 'q-bio.CB',\n",
    "    'q-fin.CP', 'cs.CL', 'cs.DC', 'math.LO', 'math.NT', 'cs.SD', 'q-fin.TR', 'cs.CV',\n",
    "    'stat.ML', 'q-fin.EC', 'econ.EM', 'cs.CE', 'stat.CO', 'math.PR', 'q-bio.NC', 'math.AP',\n",
    "    'cs.OS', 'cs.NI', 'cs.IT', 'cs.PL', 'cs.GT', 'cs.DM', 'math.IT', 'cs.SE', 'cs.RO',\n",
    "    'stat.TH', 'cs.DB', 'math.ST', 'q-bio.GN', 'q-fin.PM', 'q-bio.TO', 'math.GR', 'cs.IR'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3f7d3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:48.112689Z",
     "iopub.status.busy": "2024-02-09T19:54:48.112205Z",
     "iopub.status.idle": "2024-02-09T19:54:48.119072Z",
     "shell.execute_reply": "2024-02-09T19:54:48.117341Z"
    },
    "id": "K6gG90AxlvEP",
    "papermill": {
     "duration": 0.187558,
     "end_time": "2024-02-09T19:54:48.121818",
     "exception": false,
     "start_time": "2024-02-09T19:54:47.934260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def extract_labels(sample):\n",
    "#     \"\"\"\n",
    "#     Extract labels from the sample string and return them as a set.\n",
    "\n",
    "#     Args:\n",
    "#     - sample (str): The sample string containing labels separated by commas.\n",
    "\n",
    "#     Returns:\n",
    "#     - label_set (set): A set containing the extracted labels.\n",
    "#     \"\"\"\n",
    "#     label_set = set(sample.split(','))\n",
    "#     return label_set\n",
    "\n",
    "\n",
    "# def multi_hot_encoding(labels, vocab):\n",
    "#     \"\"\"\n",
    "#     Perform multi-hot encoding on the extracted labels based on the given vocabulary.\n",
    "\n",
    "#     Args:\n",
    "#     - labels (set): A set containing the extracted labels.\n",
    "#     - vocab (set): The set of words in the vocabulary.\n",
    "\n",
    "#     Returns:\n",
    "#     - encoding (numpy array): The multi-hot encoding scheme for the labels.\n",
    "#     \"\"\"\n",
    "#     encoding = [1 if label in labels else 0 for label in vocab]\n",
    "#     return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc502276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:48.471685Z",
     "iopub.status.busy": "2024-02-09T19:54:48.470935Z",
     "iopub.status.idle": "2024-02-09T19:54:48.476101Z",
     "shell.execute_reply": "2024-02-09T19:54:48.474861Z"
    },
    "id": "K4e-mVbtmThH",
    "papermill": {
     "duration": 0.183215,
     "end_time": "2024-02-09T19:54:48.478571",
     "exception": false,
     "start_time": "2024-02-09T19:54:48.295356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoded_labels = pd.DataFrame(columns=desired_column_order)\n",
    "# encoded_labels[\"Id\"] = test_df[\"Id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6da48321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:48.832208Z",
     "iopub.status.busy": "2024-02-09T19:54:48.831760Z",
     "iopub.status.idle": "2024-02-09T19:54:48.836425Z",
     "shell.execute_reply": "2024-02-09T19:54:48.835165Z"
    },
    "id": "C40NeXqRnSce",
    "papermill": {
     "duration": 0.184216,
     "end_time": "2024-02-09T19:54:48.839098",
     "exception": false,
     "start_time": "2024-02-09T19:54:48.654882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# predicted_categories = pd.DataFrame(predicted_categories)\n",
    "# predicted_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "415cbcdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:49.197274Z",
     "iopub.status.busy": "2024-02-09T19:54:49.196819Z",
     "iopub.status.idle": "2024-02-09T19:54:49.202191Z",
     "shell.execute_reply": "2024-02-09T19:54:49.200678Z"
    },
    "id": "AzCwa5hGmkGy",
    "papermill": {
     "duration": 0.186178,
     "end_time": "2024-02-09T19:54:49.204948",
     "exception": false,
     "start_time": "2024-02-09T19:54:49.018770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for category in encoded_labels.columns[1:]:\n",
    "#     # Check if each category is present in the 'Categories' column\n",
    "#     encoded_labels[category] = predicted_categories.apply(lambda x: 1 if category in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5411c00d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:49.556846Z",
     "iopub.status.busy": "2024-02-09T19:54:49.556408Z",
     "iopub.status.idle": "2024-02-09T19:54:49.561699Z",
     "shell.execute_reply": "2024-02-09T19:54:49.560262Z"
    },
    "id": "TTVJflm9mpWS",
    "papermill": {
     "duration": 0.183693,
     "end_time": "2024-02-09T19:54:49.564399",
     "exception": false,
     "start_time": "2024-02-09T19:54:49.380706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "004ffb25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:49.913723Z",
     "iopub.status.busy": "2024-02-09T19:54:49.913298Z",
     "iopub.status.idle": "2024-02-09T19:54:49.921359Z",
     "shell.execute_reply": "2024-02-09T19:54:49.919922Z"
    },
    "id": "7rfbjs2SnZnu",
    "outputId": "46393440-c1f4-4d30-f3e4-f80981ac0ce9",
    "papermill": {
     "duration": 0.186201,
     "end_time": "2024-02-09T19:54:49.923931",
     "exception": false,
     "start_time": "2024-02-09T19:54:49.737730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cs.LG', 'cs.CV', 'eess.IV']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_categories[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6679997",
   "metadata": {
    "id": "nBoDvgz1_NgT",
    "outputId": "a4752752-1c1b-49e2-b778-dda5d60ab281",
    "papermill": {
     "duration": 0.177415,
     "end_time": "2024-02-09T19:54:50.277137",
     "exception": false,
     "start_time": "2024-02-09T19:54:50.099722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb1ba4c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:50.631049Z",
     "iopub.status.busy": "2024-02-09T19:54:50.630406Z",
     "iopub.status.idle": "2024-02-09T19:54:50.924377Z",
     "shell.execute_reply": "2024-02-09T19:54:50.922334Z"
    },
    "id": "x6rLOJnbKGBA",
    "papermill": {
     "duration": 0.473005,
     "end_time": "2024-02-09T19:54:50.928737",
     "exception": false,
     "start_time": "2024-02-09T19:54:50.455732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Create a MultiLabelBinarizer and fit on unique categories\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(predicted_categories)\n",
    "\n",
    "# Transform the predicted_categories into binary representation\n",
    "predicted_binary = mlb.transform(predicted_categories)\n",
    "\n",
    "# Create the predicted_df DataFrame\n",
    "predicted_df = pd.DataFrame(predicted_binary, columns=mlb.classes_, index=test_df.index)\n",
    "\n",
    "\n",
    "# Concatenate the original test_df with the predicted_df\n",
    "result_df = pd.concat([test_df, predicted_df], axis=1)\n",
    "\n",
    "# Drop the 'Title' and 'Abstract' columns\n",
    "result_df = result_df.drop(['CONTEXT','Categories'], axis=1)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "result_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdbbddde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:51.301519Z",
     "iopub.status.busy": "2024-02-09T19:54:51.301018Z",
     "iopub.status.idle": "2024-02-09T19:54:51.307624Z",
     "shell.execute_reply": "2024-02-09T19:54:51.306225Z"
    },
    "id": "Xw_XTDbn4fMR",
    "papermill": {
     "duration": 0.195228,
     "end_time": "2024-02-09T19:54:51.310941",
     "exception": false,
     "start_time": "2024-02-09T19:54:51.115713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SimpleMultiLabelBinarizer:\n",
    "#     def __init__(self):\n",
    "#         self.classes_ = []\n",
    "\n",
    "#     def fit(self, Y):\n",
    "#         \"\"\"Fit the binarizer to the unique categories in Y\"\"\"\n",
    "#         unique_classes = set(cls for sublist in Y for cls in sublist)\n",
    "#         self.classes_ = sorted(list(unique_classes))\n",
    "\n",
    "#     def transform(self, Y):\n",
    "#         \"\"\"Transform the categories in Y to a binary matrix\"\"\"\n",
    "#         binary_matrix = []\n",
    "#         for labels in Y:\n",
    "#             row = [int(cls in labels) for cls in self.classes_]\n",
    "#             binary_matrix.append(row)\n",
    "#         return binary_matrix\n",
    "\n",
    "#     def fit_transform(self, Y):\n",
    "#         \"\"\"Fit to the data, then transform it\"\"\"\n",
    "#         self.fit(Y)\n",
    "#         return self.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88e1240e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:51.677319Z",
     "iopub.status.busy": "2024-02-09T19:54:51.676852Z",
     "iopub.status.idle": "2024-02-09T19:54:51.684819Z",
     "shell.execute_reply": "2024-02-09T19:54:51.683554Z"
    },
    "id": "tcYWpzqcIncP",
    "papermill": {
     "duration": 0.192856,
     "end_time": "2024-02-09T19:54:51.687587",
     "exception": false,
     "start_time": "2024-02-09T19:54:51.494731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['q-fin.PR', 'q-fin.MF']), list(['cs.DC']),\n",
       "       list(['cs.LG', 'cs.CV', 'eess.IV']), ..., list(['cs.CV']),\n",
       "       list(['cs.LG', 'cs.AI', 'cs.CV', 'cs.RO']),\n",
       "       list(['cs.LO', 'cs.SE'])], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc52df6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T19:54:52.045533Z",
     "iopub.status.busy": "2024-02-09T19:54:52.045008Z",
     "iopub.status.idle": "2024-02-09T19:54:53.631478Z",
     "shell.execute_reply": "2024-02-09T19:54:53.629346Z"
    },
    "id": "XwWXqKzaIsg7",
    "papermill": {
     "duration": 1.767038,
     "end_time": "2024-02-09T19:54:53.633127",
     "exception": true,
     "start_time": "2024-02-09T19:54:51.866089",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m predicted_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/submission.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m desired_column_order \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmath.AT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstat.AP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcs.AR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmath.QA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq-bio.MN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meess.AS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meess.IV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstat.ME\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecon.GN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meess.SP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq-fin.RM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcs.LG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcs.CR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq-bio.BM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq-fin.GN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq-fin.MF\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstat.TH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcs.DB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmath.ST\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq-bio.GN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq-fin.PM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq-bio.TO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmath.GR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcs.IR\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Reorder columns based on the desired order\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/submission.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predicted_df = pd.read_csv('/content/submission.csv')\n",
    "\n",
    "desired_column_order = [\n",
    "    'Id', 'math.AT', 'stat.AP', 'cs.AR', 'math.QA', 'q-bio.MN', 'eess.AS', 'eess.IV', 'stat.ME',\n",
    "    'econ.GN', 'eess.SP', 'q-fin.RM', 'cs.LG', 'cs.CR', 'q-bio.BM', 'q-fin.GN', 'q-fin.MF',\n",
    "    'q-fin.PR', 'math.CV', 'cs.LO', 'econ.TH', 'math.CO', 'cs.AI', 'math.AC', 'q-bio.CB',\n",
    "    'q-fin.CP', 'cs.CL', 'cs.DC', 'math.LO', 'math.NT', 'cs.SD', 'q-fin.TR', 'cs.CV',\n",
    "    'stat.ML', 'q-fin.EC', 'econ.EM', 'cs.CE', 'stat.CO', 'math.PR', 'q-bio.NC', 'math.AP',\n",
    "    'cs.OS', 'cs.NI', 'cs.IT', 'cs.PL', 'cs.GT', 'cs.DM', 'math.IT', 'cs.SE', 'cs.RO',\n",
    "    'stat.TH', 'cs.DB', 'math.ST', 'q-bio.GN', 'q-fin.PM', 'q-bio.TO', 'math.GR', 'cs.IR'\n",
    "]\n",
    "\n",
    "# Reorder columns based on the desired order\n",
    "predicted_df = predicted_df[desired_column_order]\n",
    "\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "predicted_df.to_csv('lohit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cde1c1",
   "metadata": {
    "id": "kqkjFh8cXldf",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(desired_column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a3dd5",
   "metadata": {
    "id": "mvWudHfZpEXi",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"submissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ade7b",
   "metadata": {
    "id": "pJ3IIsxapHX2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d068c",
   "metadata": {
    "id": "ByuECzSlJlRX",
    "outputId": "07e428cf-ee34-41d9-c728-e914ae3bc9c8",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv('/content/submission1.csv')\n",
    "df2 = pd.read_csv('/content/submissions (11).csv')\n",
    "\n",
    "# Ensure the column names match\n",
    "if list(df1.columns) == list(df2.columns):\n",
    "    # Compare the DataFrames\n",
    "    concatenated = pd.concat([df1, df2]).drop_duplicates(keep=False)\n",
    "    #differences = df1.compare(df2)\n",
    "\n",
    "    num_differences = len(concatenated) / 2  # Divide by 2 because each difference appears twice in the concatenation\n",
    "    print(f\"Number of different elements (rows): {int(num_differences)}\")\n",
    "else:\n",
    "    print(\"The column namesÂ doÂ notÂ match.\")\n",
    "\n",
    "#     # Check if there are any differences\n",
    "#     if not differences.empty:\n",
    "#         print(\"Differences found:\")\n",
    "#         print(differences)\n",
    "#     else:\n",
    "#         print(\"The files are identical in terms of data.\")\n",
    "# else:\n",
    "#     print(\"The column namesÂ doÂ notÂ match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b884417f",
   "metadata": {
    "id": "P1-E1r7UKm-U",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4414200,
     "sourceId": 7582942,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4498.950298,
   "end_time": "2024-02-09T19:54:56.836228",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-09T18:39:57.885930",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
